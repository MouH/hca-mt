\documentclass[utf8,english]{gradu3}
% If you are writing a Bachelor's Thesis, use the following instead:
%\documentclass[utf8,bachelor,english]{gradu3}

\usepackage{booktabs} % good for beautiful tables
\usepackage{todonotes}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb}
%some hacking because both the template and the amsmath package define \proof
\let\proof\relax 
\let\endproof\relax
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{paralist}
\usepackage{mdwlist}
\usepackage{threeparttable}
\usepackage{footmisc}
\usepackage{soul}
\usepackage{cleveref}
\usepackage{varwidth}
\usepackage{enumitem}



% NOTE: This must be the last \usepackage in the whole document!
\usepackage[bookmarksopen,bookmarksnumbered,linktocpage]{hyperref}

\addbibresource{malliopas.bib} % The file name of your bibliography database


\begin{document}

\title{Clustering Analysis and Approximate Hierarchy Clustering Algorithm}
\translatedtitle{\LaTeX-tutkielmapohjan {gradu3} käyttö}
\studyline{All study lines}
\avainsanat{%
  \LaTeX,
  {gradu3},
  pro gradu -tutkielmat,
  kandidaatintutkielmat,
  käyttöohje}
\keywords{\LaTeX, {gradu3}, Master's Theses, Bachelor's Theses, user's guide}
\tiivistelma{%
  Empty TODO
}
\abstract{%
  Clustering Analysis and Approximate Hierarchy Clustering Algorithm.
}

\author{Mou Hao}
\contactinformation{Ag~C416.1, 
	\texttt{mouhao1990@outlook.com}}
% use a separate \author command for each author, if there is more than one
\supervisor{Unsupervised work}
% use a separate \supervisor command for each supervisor, if there
% is more than one

 % you don't need this line in a thesis
% \type{Template and manual for a thesis document class}

\maketitle

\preface
This is where you can write a preface for your thesis.  Most theses
don't have prefaces, but if you write one, keep it short (at least one
page).

The preface should discuss more the thesis process than the content of
the thesis.  For example, if there is something out of the ordinary in
your choice of a thesis topic or if something out of the ordinary
happened during its prepararion, the preface is where you could write
about it.  It is also customary in a preface to thank by name those
persons who helped you with your thesis -- at least your supervisor,
your spouse and your children, if any.  (Your family likely will have
helped you by encouraging and supporting you.)

The preface is typically in the first person (``I'').  It is also common
to sign it.

Jyväskylä, \today

\bigskip

The Author

\begin{thetermlist}
\item[\TeX] A batch-oriented typesetting system written by 
Donald Knuth in 1977--1989 \parencite[see][]{knuth86:_texbook}. 
\item[\LaTeX] A system, built on top of \TeX\
  \parencite{knuth86:_texbook}, for typesetting structured
  documents \parencite[see][]{lamport94:_latex}.  Its current version
  is \LaTeXe.
\end{thetermlist}

\mainmatter

\chapter{Introduction}

Introduction (describe the structure of the thesis and point out the contributions)

\chapter{Clustering}

Clustering (includes normal and hierarchical clustering)

The clustering methods are mainly divided into two categories viz., partition based clustering and hierarchical clustering, based on the way they produce the results.

\section{Similarity Measure}

Similarity measure, which is the essential part of any clustering algorithm, a brief but concrete discussion about similarity measure is conducted in this section.

\section{Partial Clustering}

Partial clustering:
- distance based 
	k-means
- density based

\section{Hierarchical Clustering Algorithm}

discuss about the basic idea of hierarchical clustering algorithm.
discuss the usage of HCA
show the basic single  average linkage algorithm by naive code.

\algrenewcommand\Return{\State \algorithmicreturn{} }% http://tex.stackexchange.com/questions/69449/avoid-putting-statements-on-the-same-line-with-algorithmicx
\begin{algorithm}[h]
	\caption{Primitive AHC algorithm}
	\label{alg:primitiveHAC}
	\begin{algorithmic}[1]
		\Procedure{primitive AHC}{s,d}
		
		\State $S_{origin} \leftarrow S$
		\State $n \leftarrow \lvert S \rvert$
		\State $den \leftarrow \left[\right]$
		\State $size[x] \leftarrow 1, $ for all $x \in S$
		\For{$i \leftarrow 0, \dots , n - 2 $}
		\State $(I,J) = argmin_{(SxS)\setminus \Delta} d$
		\State append (I,J) to den
		\State $S \leftarrow S \setminus \left\{I,J\right\} $
		\State Create a new label $L$, $L \notin S \cup S_{origin}$
		\State \begin{varwidth}[t]{\linewidth}
			Update the matrix containing the distances
			\par\hskip\algorithmicindent $d[L,x] = d[x,L] = \text{FORMULA} ( d[I,x], d[b,J],$ 
			\par\hskip\algorithmicindent $d[I,J], size[I], size[J]) $,  for all $x \in S$
		\end{varwidth}
		\State $size[L] \leftarrow size[I] + size[J]$
		\State $S \leftarrow S \cup \left\{L\right\}$
		\EndFor 
		
		\Return den
		\EndProcedure
		
		The $\text{FORMULA}$ is the updating formula used for the chosen linkage, while $d$ is the distance metric. Note that our notation somewhat freely uses $I$ and $J$ to mean either the label of the cluster or the cluster itself.
	\end{algorithmic}
\end{algorithm}

\chapter{Approximate Hierarchical Clustering Algorithm}

As discussed in the previous chapter, the time complexity of a traditional accurate hierarchical clustering algorithm is $O(n^3)$. However, the datasets size of today's application domain has dramatically increased. Conducting a traditional agglomerative hierarchical clustering on such dataset would not be proper for an expected running time.

To handle the scaling challenge, researchers found two approaches. At fist, researchers are focused on how to find faster algorithms which generate the same hierarchical tree as the original algorithm. The hardworking of the first approach came to its limit, when dataset scale becomes even lager. The later contribution turns to find a approximate hierarchical clustering algorithm, which is not consistency to but closely resembles the exact algorithms. 

This chapter will mainly discuss the current research in the approximate hierarchical clustering algorithms. 

\section{Experiment and Comparison Difficulty}

Before the discussion of the approximate algorithms, an brief illustration about the difficulty of experiment and comparison of such approximate hierarchical clustering algorithms.

%TODO

\section{Algorithm by Patra}

Patra et al.~\cite{patra2010distance}  proposed a method, l-AL for AHC to deal with large dadtasets problem with average linkage. In their work, a set of leaders are proposed to represent the whole datasets, which are then applied the standard average link method. The advantage is that this method works for any distance metric, and reduces the running complexity as it is not requested to store the whole  dataset in to memory, only the leaders are retained. 

To perform a $l-AL$ algorithm, firstly, a set of leaders should be chose, and a standard average linkage method will be implement inside each group. The average linkage method has been discussed in the previous chapter. The focus below will be mainly about the way they used to choose leaders from the origin dataset.

\algrenewcommand\Return{\State \algorithmicreturn{} }
\begin{algorithm}[h]
	\caption{Leaders Selecting algorithm}
	\label{alg:selectLeader}
	\begin{algorithmic}[1]
		\Procedure{leaderSelect}{dataset, $\tau$}
		\State $leaders \leftarrow empty hash map of node and set of nodes$
		\For{$i \leftarrow dataset$}
			\State \text{IF}  exists a $l$ in $leaders$, and $ || l - i || < \tau $
			\State \hskip\algorithmicindent put current node $i$ into the set in $leaders$ with key $i$.
			\State \text{ELSE}
			\State \hskip\algorithmicindent make i a new leader, put an empty set into $leaders$ with key $i$
		\EndFor		
		\Return $leaders$
		\EndProcedure

	This algorithm takes two input, one is the whole dataset which is used in the hierarchical clustering, the other one is a tolerante value $\tau$. The return value is the chosen leaders with their attached nodes.
	\end{algorithmic}
\end{algorithm}

A traditional way of finding leaders is shown in Algorithm\ref{alg:selectLeader}. The size of leaders is determined by the dataset itself and also largely on the parameter $\tau$. The time complexity is $O(mn)$.

In the article by Patra et al.~\cite{patra2010distance}, the author introduced a more efficient way to compute the leaders. An algorithm called $Accelerating Leaders$ is proposed, which take the use of triangle inequality property and reduce the iteration when computing leaders. The detail of this $Accelerating Leaders$ algorithm is not mentioned here, as it is not the essential part of this paper, one can easily refer to the paper.

The $l$-AL algorithm is based on the leader selection and average linkage to perform the clustering. 
Algorithm \ref*{alg:lal} shows how to perform the $l$-AL, with a time complexity $O(mn + m^2)$ = $O(mn)$ and a space complexity $O(m^2)$.
Compared to the traditional AL algorithm, $l$-AL algorithm does not perform the hierarchy clustering method on the whole dataset, which reduce the computation pressure. 
As the author discussed about, the $l$-AL may overestimate or underestimate the distance between different clusters.
However, for larger datasets, the members of each leader is also big, which will make the members will distributed.


\algrenewcommand\Return{\State \algorithmicreturn{} }
\begin{algorithm}[h]
	\caption{$l$-AL}
	\label{alg:lal}
	\begin{algorithmic}[1]
		\Procedure{$l$-AL}{$D$, $\tau$, $h$}
		\State Apply leader selection algorithm \ref{alg:selectLeader} on $D$, get a map $leadersMap$, get keys out from $leadersMap$ as $L$.
		\State Apply average linkage algorithm AL($L$, $h$), to get a hierarchical cluster $HC$.
		\State Replace each leader in $HC$ with a set of nodes in $leadersMap$
		\State Thus, $HC$ becomes a sequence of clustering dataset $result$
		\Return $result$
		\EndProcedure
		
		This algorithm takes three inputs, a raw dataset $D$, two tuning parameters, $tau$ and $h$.
	\end{algorithmic}
\end{algorithm}



\section{Algorithm by Gilpin}
3.2 Algorithm by Gilpin et. al

\section{Twister Tries Approach}
3.3 Twister tries + the extension I worked on later

\section{Comparison}
3.4 Some comparison (this can also be done directly in the section, but it might be easier to do it separatly)

Appendix: the article

\end{document}
